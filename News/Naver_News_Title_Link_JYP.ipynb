{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 네이버 뉴스 제목, 링크 크롤링\n",
    "#### 마지막 실행일자 : 23.11.05\n",
    "#### ※ Selenium 3.x 버전에서 실행 및 테스트함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests as req\n",
    "from selenium import webdriver as wb\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup as bs #태그 형태를 읽어 올수 있는 라이브러리 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 사용자 함수\n",
    "# 페이지 넘기는 함수\n",
    "def setting(idx):\n",
    "    try:\n",
    "        url = f\"url_input\"# url 설정url 설정\n",
    "        header = {'user-agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36\"}\n",
    "        res = req.get(url, params={'query':'신천지'}, headers = header) #request 에서 url 값 읽어오기\n",
    "        soup = bs(res.text, 'html.parser') #soup 라이브러리를 html.parser로 연결하기\n",
    "        driver.get(url)\n",
    "        driver.find_element_by_class_name(\"btn_next\").click()         \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    return soup\n",
    "\n",
    "\n",
    "# 네이버뉴스만 가져오기\n",
    "def appendData(a_news_tit_tag, a_info_tag):  \n",
    "    news_tit_tag = []\n",
    "    href_tag = []\n",
    "    \n",
    "    for news_tit in a_news_tit_tag:\n",
    "        for info in a_info_tag:        \n",
    "            if info['href'].startswith(\"https://n.news.naver.com/\") and info['class']==['info'] and info['href'] not in href_tag:\n",
    "                url = info['href']\n",
    "                res = req.get(url, params={'query':'input'}, headers = header) #request 에서 url 값 읽어오기\n",
    "                driver.get(url)\n",
    "                soup = bs(res.text, 'html.parser')\n",
    "                news_tit_tag.append(soup.find(\"title\").text)\n",
    "                href_tag.append(info['href'])\n",
    "                break\n",
    "                \n",
    "    return news_tit_tag, href_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## 크롤링\n",
    "# 크롬열기\n",
    "driver = wb.Chrome()\n",
    "url = \"url_input\" # url 설정\n",
    "header = {'user-agent':\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3809.100 Safari/537.36\"}\n",
    "res = req.get(url, params={'query':'input'}, headers = header) #request 에서 url 값 읽어오기\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "time_tag_list = []\n",
    "news_tit_tag_list = []\n",
    "href_tag_list = []\n",
    "\n",
    "SIZE = 200  # 크롤링할 페이지 번호 선택\n",
    "# for i in range(0, SIZE, 10):\n",
    "for i in range(100, SIZE):\n",
    "    print('-------------------------------', i+1, '페이지 크롤링 시작')\n",
    "    soup = setting(i)      \n",
    "    time_tag = []\n",
    "    a_info_tag = soup.select('div.info_group > a.info')\n",
    "    span_info_tag = soup.findAll('span', class_='info')\n",
    "    for a_info, span_info in zip(a_info_tag, span_info_tag):\n",
    "        if a_info.find(\"i\") != None or span_info.find(\"i\") != None:\n",
    "            continue\n",
    "        else:\n",
    "            time_tag.append(span_info.text)\n",
    "            \n",
    "    a_news_tit_tag = soup.findAll('a', class_=\"news_tit\") #soup 라이브러리로 제목 읽어 오기\n",
    "    news_tit_tag, href_tag = appendData(a_news_tit_tag, a_info_tag)\n",
    "\n",
    "    for time, news_tit, href in zip(time_tag, news_tit_tag, href_tag):\n",
    "        time_tag_list.append(time)\n",
    "        news_tit_tag_list.append(news_tit)\n",
    "        href_tag_list.append(href)\n",
    "        print(\"일자:\", time, \"\\n\", \"제목:\", news_tit, \"\\n\",  \"url:\", href)  \n",
    "    print('-------------------------------', i+1, '페이지 크롤링 완료')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic={\"시간\":time_tag_list, \"제목\": news_tit_tag_list,  \"링크\": href_tag_list}\n",
    "data = pd.DataFrame(dic)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"네이버_뉴스.csv\", encoding=\"utf-8-sig\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
